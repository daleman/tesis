%Acá explicamos cómo extrajimos los datos, qué características tiene la muestra (muchos de los análisis/gráficos ya los hicimos), cómo la separamos, y qué análisis estadísticos estamos realizando (es la parte que nos falta)

\section{Extracción de Datos}

Para extraer los tweets se utilizó la librería de \textit{python} llamada \textit{tweepy}. Con ella primero se extrajo una cantidad de usuarios de forma localizada para después extraer tweets de estos.
Los usuarios se buscaron por provinicia para tener una cantidad de usuarios aproximadamente equitativa.
La búsqueda de los usuarios se hizo de la siguiente manera:
Por cada provincia de la Argentina, se extrajo las ubicaciones de los departamentos de cada provincia, de los partidos de la provincia de buenos aires y de las comunas de la ciudad de buenos aires. El conjunto de estas forman la subdivisión de segundo orden de la republica Argentina. La lista de departamentos/partidos/comunas fue extraida a partir de los datos publicados del Censo Argentino realizado en el año 2010. 
Los departamentos y partidos de la Argentina son la subdivisión de 2.º orden de las provincias de la Argentina. En todas ellas se las denomina departamentos, excepto en la provincia de Buenos Aires donde por razones históricas se las llama partidos. La Ciudad Autónoma de Buenos Aires se organiza en comunas (no tienen nombre, sino que se denominan por número, del 1 al 15).


% La cantidad de usuarios debería ser igual para todas las provincias, o debería ser en función de la población de la provincia.

\subsection{Búsqueda geolocalizadas}
Una vez obtenida esta lista de ubicaciones, por cada provincia se realizaron búsquedas con centro en las coordenadas de los departamentos de la misma y con un radio de 20 millas. Sobre el resultado de esta búsqueda, únicamente se seleccionaron los usuarios que tienen como campo \textit{location} al menos uno de los nombres de las ciudades de la provincia.

Hubo varios problemas con las búsquedas localizadas:
\begin{itemize}
    \item No todos los usuarios tienen geolocalización activada.
    \item Dentro de los que tienen geolocalización activada, no todos viven allí (posibilidad de turistas)
    \item Las búsquedas geolocalizadas también dan como resultados los tweets que son retweets de personas que tienen la ubicación solicitada.
\end{itemize}
%Revisar%
El primer problema, solo afecta a la cantidad de tweets que se pueden recolectar.
El segundo y el tercer problema, son solucionados con el chequeo del campo \textit{location}.
%When conducting geo searches, the search API will first attempt to find Tweets which have lat/long within the queried geocode, and in case of not having success, it will attempt to find Tweets created by users whose profile location can be reverse geocoded into a lat/long within the queried geocode, meaning that is possible to receive Tweets which do not include lat/long information.
Las búsquedas geolocalizadas de la API de \textit{twitter} primero intentan de buscar Tweets cuyas coordenadas sean las que fueron las buscadas, y en caso de no tener éxito, buscará los Tweets creados por usuarios que tienen en el campo textit{location} de su perfil un lugar cuyo geocódigo coincida con el de sus coordenadas. Es decir, si se hace una búsqueda inversa de las coordenadas, devuelve el lugar de su perfil.  

A continuación se muestra un gráfico con las ubicaciones donde se encuentran los usuarios:

% \begin{figure}[ht]
% \centering
% \includegraphics[scale=0.6]{imagenes/ubicacion_usuarios.png}
% \caption{Ubicaciones de los usuarios} 
% \label{fig:busqueda_usuarios} 
% \end{figure}


De esta manera se recolectó aproximadamente 2000 usuarios por provincia lo que resume en 46000 usuarios argentinos. Sobre este conjunto de usuarios se buscaron los tweets realizados por estos. Se decidió no tener en cuenta los retweets dado que estos no son escritos por los usuarios si no que son una mera copia de otros tweets. %Buscar mejor justificación

\subsection{Datos de entrenamiento y de validación}

Por cada provincia se tomó a los usuarios de la misma y se los dividió para tener un conjunto de datos de entrenamiento y uno de validación.
La división fue de la siguiente manera:
Sobre el conjunto de usuarios se dividió en dos de forma aleatoria, obteniendo $Usuarios_1$ y $Usuarios_2$. Luego se buscó la fecha $Fecha_{Div}$ por la cual había una cantidad equiparable entre el conjunto de tweets producidos por  $Usuarios_1$ antes de $Fecha_{Div}$ y el conjunto de tweets producidos por $Usuarios_2$ despúes de $Fecha_{Div}$.Es decir:

\begin{equation}
\sum_{ f = FechaInicial}^{Fecha_{Div}} tweets(Usuarios_1,f) \approx \sum_{ f = FechaInicial}^{Fecha_{Div}} tweets(Usuarios_2,f) 
\end{equation}

Después de saber la fecha se dividió al conjunto de tweets producidos por estos usuarios, con el conjunto de entrenamiento con los tweets producidos antes de $Fecha_{Div}$ y el conjunto de test producidos posteriormente a esa fecha.

\section{Tokenización y Normalización}

Dados los textos, hubo que realizar una limpieza de estos debido a que en \textit{Twitter} ocurren palabras que contienen números dentro de ellas, emoticones y signos de puntuación. Luego se decidió tomar como palabra, aquellas secuencias de caracteres separadas por espacios que no contienen números, signos de puntuación ni emoticones, es decir solo las secuencias de caracteres alfabéticos. 
Además de la tokenización del texto, se realizó una normalización sobre él. Todas las letras se llevaron a letra minúscula y las palabras con más de tres letras repetidas se redujeran para que solo tengan tres repeticiones. Esto se realizó con la librería \textit{TweetTokenizer de NLTK}. 

\section{Caracterización de la muestra}

Para tener una noción más completa de la muestra, presentamos una serie de gráficos que muestran las cantidades de palabras y tweets por provincia.

% \begin{table}[]
% \centering
% \caption{Cantidades del dataset}
% \label{my-label}
% \begin{tabular}{lllllll}
% Provincia      & cantPalabras & cantUsuarios & cantTweets & cantTotal & Primer tweet & Último tweet \\
% Buenos Aires    & 191919       & 920          & 1125042    & 8974372   & 2007-09-20   & 2016-03-11   \\
% Catamarca      & 173104       & 957          & 1057019    & 8161309   & 2007-06-15   & 2015-11-30   \\
% Chaco          & 169476       & 964          & 976943     & 7605991   & 2009-09-18   & 2016-05-13   \\
% Chubut         & 182592       & 954          & 1023373    & 8884745   & 2009-08-03   & 2016-07-12   \\
% Córdoba        & 207307       & 987          & 1224266    & 10075932  & 2009-03-05   & 2016-06-28   \\
% Corrientes     & 183292       & 939          & 1044951    & 8426940   & 2009-08-11   & 2016-06-19   \\
% Entre Ríos      & 188679       & 969          & 1193693    & 9462986   & 2009-07-16   & 2016-06-23   \\
% Formosa        & 169254       & 903          & 923352     & 7184382   & 2009-08-09   & 2016-05-20   \\
% Jujuy          & 171064       & 971          & 678004     & 5951778   & 2008-04-17   & 2015-08-25   \\
% La Pampa        & 186593       & 935          & 1085757    & 8996318   & 2009-04-21   & 2016-05-12   \\
% La Rioja        & 186041       & 946          & 704044     & 6757277   & 2009-04-13   & 2015-08-06   \\
% Mendoza        & 193708       & 945          & 1099717    & 9402399   & 2009-01-14   & 2016-07-18   \\
% Misiones       & 168400       & 972          & 984218     & 7790197   & 2009-07-03   & 2016-05-30   \\
% Neuquen        & 188038       & 927          & 1111201    & 9021449   & 2009-09-24   & 2016-07-23   \\
% Río Negro       & 194383       & 965          & 1215361    & 9991831   & 2009-11-21   & 2016-06-01   \\
% Salta          & 188402       & 884          & 830916     & 7506652   & 2009-05-13   & 2016-02-24   \\
% San Juan        & 183546       & 926          & 1002322    & 8377792   & 2009-06-19   & 2016-06-14   \\
% San Luis        & 164185       & 896          & 1006464    & 8327093   & 2009-07-01   & 2016-05-26   \\
% Santa Cruz      & 174089       & 935          & 876621     & 7432923   & 2009-05-20   & 2015-12-30   \\
% Santa Fe        & 201879       & 937          & 1019620    & 8862328   & 2009-05-11   & 2016-04-13   \\
% Santiago del Estero       & 166540       & 887          & 944109     & 7355729   & 2009-07-05   & 2016-05-25   \\
% Tierra del Fuego & 197273       & 964          & 976426     & 8559218   & 2008-05-17   & 2016-02-24   \\
% Tucuman        & 195643       & 962          & 1093874    & 9238526   & 2009-01-21   & 2016-06-15  
% \end{tabular}
% \end{table}


% Cantidad de usuarios por provincia, en train y en test
% cantidad de tweets por usuario, cantidad media de palabras por usuario
% Cantidad de tweets por fecha en el conjunto de datos 

% Ley de zipf, ver que vale con nuestro dataset

% las metricas para encontrar las palabras con contrastes

\subsection{Métricas para medir el contraste en la frecuencia de las palabras}

\subsubsection{La entropía como médida del desorden}

Para ver la cantidad de información que nos aporta cada palabra se hará una introducción a la teoría de la información, especificamente
los conceptos que introdujo Claude Shannon\cite{shannon2001mathematical}.
Para entender estos conceptos es útil tener una descripción matemática del mecanismo que genera la información. Para eso se define a 
la \textit{fuente} que emite señales de un alfabeto $ S = \{s_1, s_2, \dots\, s_q\}$ de acuerdo a una función de probabilidad fija.
Si la fuente emite señáles estadísticamente independientes decimos que es una \textit{fuente de memoria nula} y un símbolo $s$ está completamente determinado por el alfabeto $S$ y las probabilidades:
$P(s_1)\,P(s_2)\, \dots\, P(s_q)$

Sea X una variable aleatoria discreta con posibles valores $\{x_1, x_2, \dots\, x_q\}$ y una función de probabilidad P(X), luego:
${\displaystyle \mathrm {H} (X)=\mathrm {E} [\mathrm {I} (X)]=\mathrm {E} [-\log(\mathrm {P} (X))].}$
donde X es una variable aleatoria con posibles valores $\{x_1, ... , x_n\}$ y P es una función de probabilidad.

Los símbolos con menor probabilidad son los que aportan más información. Esto va de la mano con nuestra intuición ya que si entendemos a los símbolos como palabras de un texto, las palabras más utilizadas como \textit{de} o \textit{que} aportan menos información que la palabra \textit{celular}. 
Observaciones:
\begin{itemize}
    \item La entropía es máxima cuando los eventos de X son equiprobables. En este caso, si hay n eventos con una probabilidad de $\frac{1}{n}$ cada uno , el valor de la entropía es de $\log n$.
    \item La entropía es 0 si y solo sí todas las probabilidades son 0 a excepeción de una con probabilidad igual a la unidad. 
\end{itemize}

Dado que la entropía es máxima cuando los eventos de X son equiprobables, se suele decir que es una médida del desorden

Una médida que se puede usar para comparar las frecuencias de las palabras en las difentes regiones del país puede ser la entropía de Shannon, debido a que podemos tener un valor que informe que tan uniforme es la distribución de las frecuencias de cada palabra.
Sin embargo, la entropía como única médida tiene sus desventajas. Principalmente, una palabra con una sola ocurrencia en una provincia y ninguna en las demás tiene la entropía máxima. Debido a que nos interesan las palabras con más de una ocurrencia se trató de elaborar otrá métrica que tenga en cuenta la entropía, pero que no sea la única variable a tener en cuenta.


% entropia, defincion, de donde viene, formula
% usamos la multinomial
% valor de informacion de zanette. ejemplos de como extrae las palabras más importantes/
% nos inspiramos en el valor de informacion, explicar la formula utilizada
%

