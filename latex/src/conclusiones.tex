
\subsection{Conclusiones}
En el presente trabajo recolectamos un conjunto de datos de texto de la Argentina a través de la API de Twitter. Este conjunto lo dividimos en dos, un conjunto para desarrollar una métrica que indique el valor contrastivo de una palabra. El segundo conjunto de datos, independiente del primero en cuanto a usuarios y al período temporal de los textos lo utilizamos para hacer un test estadístico para corroborar que las palabras detectadas como contrastivas según nuestra métrica, no estaban sobrerepresentadas en el conjunto de desarrollo.

La métrica que realizamos usaba la entropía para medir la variación de la cantidad de ocurrencias y de la cantidad de usuarios que la utilizaban en las distintas provincias del país. Creamos un listado de palabras ordenado según el valor de la información calculada, y a partir de ella filtramos las palabras de forma manual eliminando los términos que no tengan valor lingüístico, como los nombres propios (como los nombres de personas, o de lugares). Varias palabras del listado, como \textit{mitaí, anga,angau} resultaron muy interesantes a nivel lingüístico


Sobre estas palabras realizamos el test estadístico. 

% Comentar que la metrica evidentemente detecta palabras contrastivas e interesantes a nivel linguistico.
% Hablar tambien de los problemas encontrados.
\subsection{Trabajo Futuro}

%Separar los nuevos desafios de las mejoras tecnicas a realizar sobre este trabajo
Uno de los desafíos que quedan para hacer es el de clasificar las regiones en clusters, obteniendo así las regiones dialectales. De esta manera se podría ver la vigencia de las regiones descriptas por Vidal de Battini. % Hacer referencia a la sección de la introdcucción que se habla de eso

El proceso de normalización se podría mejorar para tener una mejor precisión de las palabras utilizadas. También se podría agregar un sistema de reconocimiento de nombres de entidades para filtrar los nombres propios de manera tal que el listado de palabras contrastivas tengan menos términos sin interés lingüístico.

Por otro lado, este trabajo se podría realizar sobre todo el conjunto de países hispanoparlantes, de modo tal que se puedan hacer comparaciones entre los mismos y comparar las variaciones entre regiones más grandes.\\

También queda por hacer un análisis sintáctico de las oraciones, y un análisis estadístico de bigramas.
Otro desafío es el de analizar los tuits modelados por cadenas de markov (analizando así bigramas y n-gramas), pudiendo generar un bot que cree tuits, este bot podría ser parametrizado de modo tal que genere textos, teniendo en cuenta únicamente los tuits de determinada región.

