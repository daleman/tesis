%Explicación del área
%el trabajo de vidal de batini y almeida
%Objetivo de la tesis

%Contrsuccion de lingüistica de corpus
%lingüistica computacional, su evolucion
%Todo se hace en ingles y hay pocos analisis en castellano.

%primer acercamiento de contrastes léxicos 


%Comentar el problema de la lingüística de Corpus, trabajo previo en el área, comentar brevemente qué queremos hacer. Acá va a ir mucho de lo que nos pasó Santiago como literatura, incluso mencionando (por ejemplo) el trabajo de Vidal de Battini

\section{Trabajo previo en el área}
La dialectología es un campo que estudia la variación de las lenguas según la región geográfica. La investigación en estos campo suele utilizar variables lingüísticas: atributos fonéticos, sintácticos y léxicos.

%Ampliar sobre dialectologia (fijarse en la enciclopedia)
Una palabra es \textit{contrastiva} cuando la frecuencia de uso en dos regiones es muy diferente. Por ejemplo, la palabra ``che''\footnote{Tratamiento que se usa para llamar, pedir atención o dirigirle a alguien la palabra \cite{academia2008diccionario}.}, o ``metegol''\footnote{Juego mecánico en el que con pequeños muñecos se simula un partido de fútbol (futbolín) \cite{academia2008diccionario}.} son términos más usados en la Argentina que en España. Un ejemplo dentro de la Argentina, es el de ``gurisada'' conocido en el litoral y noroeste argentino como un conjunto de chicos \cite{academia2008diccionario}.
%El criterio de contrastividad es importante especialmente para realizar diccionarios contrastivos como el 
Actualmente, las palabras con contraste léxico en distintas regiones
se detectan por medio de cuestionarios como el de \emph{Almeida} \cite{almeida1995variacion} o en métodos que dependen, en mayor o menor medida, de la intuición y sensibilidad de los lexicógrafos. Las encuestas están integradas por grupos temáticos centrales como la casa, la familia, la enseñanza, el cuerpo humano, etc. Sobre cada grupo temático se les indica a las personas entrevistadas un repertorio de palabras por cada noción, para ver si las conocen y con qué frecuencia se las usa. 
Con el presente trabajo planteamos cambiar el paradigma y detectar automáticamente las palabras usadas en distintas regiones y sus frecuencias.

Uno de los corpus lingüísticos del español más reconocidos es el \emph{CORPES XXI}\cite{espanolabanco}, creado por la Real Academia Española con una distribución de 25 millones de formas por cada uno de los años comprendidos en el periodo 2001 
a 2012. Sin embargo, dicho corpus tiene dos desventajas importantes: por un
lado, la cantidad de palabras de América Latina está subrepresentada en relación a la demografía ya que el $34,30$\% de las palabras del dataset proviene de textos de España y $65,70$\% de los demás países hispanoparlantes. Por otro lado, uno no dispone de todo el dataset, sino que solamente se pueden hacer las consultas desde su página web. Estas consultas están limitadas en cuanto a la cantidad de solicitudes y a las funcionalidades que estas proveen, especialmente tener que saber de antemano la forma a buscar.

Una de las virtudes de hacer un corpus con un método de recolección de textos de forma automática se desprende del mayor tamaño del corpus en comparación con métodos manuales como digitalización de textos. A pesar de haber comenzado hace varias décadas la recolección de textos de la web para realizar corpus, no hay muchos en el idioma español.
Uno es el de \textit{Mark Davies}, en el cual se utilizaron las páginas web para recolectar los textos, con dos mil millones de palabras en español, y se dividieron las páginas a partir del país de origen identificado por \textit{Google} \cite{davies2015}. A pesar de ser un corpus muy grande con anotaciones, no permite diferenciar las frecuencias de las palabras o los textos originados en regiones dentro de cada país. Es por eso que para este trabajo elegimos construir nuestro propio conjunto de datos que contenga una gran cantidad de palabras. Para esta tarea, usamos datos de \textit{Twitter}. A continuación haremos una descripción de esta plataforma y de sus principales ventajas sobre otras alternativas posibles.


\section{Twitter}
Twitter\footnote{www.twitter.com} es un servicio de microblogging creado en 2006. Los usuarios son variados, desde personas, instituciones gubernamentales y no gubernamentales hasta bots (i.e programas que corren tareas automáticamente). Cada usuario puede escribir textos llamados tuits, que tienen una longitud máxima de 140 caracteres\footnote{Recientemente han aumentado el límite a los 280 caracteres.}. Las relaciones en Twitter no necesitan ser recíprocas. Es decir, uno puede seguir a una persona en cuyo caso va a poder leer todos los tuits generados por ella, como también puede ser seguido por una persona. Un tuit puede ser respondido, como también puede ser retuiteado. El retuit es un mecanismo para diseminar por la red tuits generados por otros usuarios. De esta manera si un usuario $A$ realiza un retuit generado por el usuario $B$, cualquier seguidor de $A$ también va a recibir ese tuit en su panel, al cual llamaremos \textit{timeline}. Si bien los tuits que se ven en el timeline son solo aquellos generados por los usuarios que uno sigue, todos los tuits son públicos, es decir que se pueden acceder a ellos a través de búsquedas en la plataforma.
Para dar una noción de la cantidad de usuarios en la Argentina, en el año 2016 había 11,8 millones de usuarios de Twitter. Teniendo en cuenta que en ese momento 15 millones de personas tenían smartphones se deduce que el $70$\% de la gente con smartphones poseía una cuenta de Twitter.%{ref de lanacion}

Las ventajas de Twitter sobre otras plataformas son varias: provee una interfaz pública para obtener tuits de cualquier persona, independientemente de que haya una relación con el usuario que escribió los tuits en la red social. Es decir, uno puede ver los tuits de otra persona, sin necesidad de ser un \textit{seguidor} de la misma. Además, a diferencia de un portal de noticias donde los comentarios suelen estar relacionados con estas, en Twitter son más amplios los tópicos de los comentarios.
Por otro lado, Twitter es una tecnología que permite hacer escalable el trabajo a diferentes países ya que con una misma interfaz se pueden obtener los textos de cualquier región. En cambio, si se elige un portal de noticias para sacar comentarios de usuarios, es necesario conocer la estructura de cada página para obtener esos datos.
Otra virtud de Twitter es la identificación de los usuarios sobre la cual se podrían inferir datos como género, edad y ubicación de cada uno.
Por último, una ventaja sobre otro método de recolección es que a través de Twitter se pueden recolectar palabras con una granularidad regional muy variable y obtener información de quienes los escriben.
Existen otras redes sociales de las que se podrían obtener textos para analizar, pero tienen la desventaja de ser privadas, como \textit{Facebook}, o acotadas en términos de los temas que se hablan, como \textit{Linkedin}. 

En los últimos años se han publicado numerosos trabajos que utilizaron datos de Twitter, desde la detección y monitoreo de terremotos en tiempo real\cite{sakaki2010earthquake}, análisis de sentimientos y de la opinión pública \cite{liu2012sentiment},  predicciones del mercado de valores \cite{pak2010twitter} o los resultados de elecciones nacionales \cite{tumasjan2010predicting}. También se ha utilizado para localizar enfermedades por región \cite{paul2011you}.

En cuanto a trabajos relacionados con la lingüística cabe mencionar el trabajo de Eisenstein et al. \cite{eisenstein2010latent} en el que identifica palabras con una gran afinidad regional realizando un modelo probabilístico en el que asumen que las distribuciones léxicas dependen de la región geográfica y de una división de tópicos. En otras palabras, suponen que hay una división de temas sobre todo el dataset y dependiendo de la región del autor, este es más propenso a escribir con una variación dada. El mismo autor realizó un trabajo para identificar variables léxicas y detectar regiones dialectales \cite{eisenstein2014identifying}.
El trabajo de Gonçalves et al. \cite{gonccalves2014crowdsourcing} consistió en analizar las variaciones diatópicas de ciertos conceptos en las grandes ciudades hispanoparlantes. Utilizaron la técnica K-means \cite{bishop2006pattern} para obtener regiones dialectales. Por otro lado G. Doyle et al. \cite{doyle2014mapping} propone un método bayesiano para estimar la distribución de la frecuencia de una palabra (o frase) condicional a la ubicación de la persona que la escribe.
Hay dos grandes diferencias entre el presente trabajo con los que se mencionaron anteriormente. La primera diferencia es que el análisis está hecho sin un enfoque bayesiano. La segunda diferencia es que este trabajo se realizó con textos en español, mientras que la mayoría de los trabajos están hechos sobre corpus en inglés. 


\section{Lingüística de Corpus y Lingüística computacional} % (fold)
\label{linguistica_computacional}

La lingüística de corpus es una rama de la lingüística que investiga a través de conjuntos de muestras de uso de la lengua \cite{mcenery2011corpus}. Aunque lo más común es que estas muestras provengan de textos, también se puede extraer datos a partir de grabaciones de voz o videos. Si bien esta rama nació analizando corpus de forma manual, el rápido crecimiento
tecnológico de las últimas décadas dio la posibilidad de tener corpus con millones de palabras y hacer algunos estudios de manera más automatizada, usando menos recursos humanos y ahorrando tiempo. A continuación, haremos un breve resumen de algunos hitos en lingüística de corpus.

En el año 1967 se publicó el primer corpus con un millón de palabras denominado Brown Corpus, uno de los pioneros en la lingüística de corpus. Recién en el año 1995 se consiguió generar un corpus del inglés británico con 100 millones de palabras, titulado British National Corpus (BNC). 
El objetivo era que se convirtiera en una muestra representativa del inglés británico de aquella época. 
Es importante destacar, por la gran cantidad de recursos que se utilizaron, que este trabajo se hizo con la colaboración de tres grandes editoriales, la Universidad de Oxford, la Universidad de Lancaster y la Biblioteca Británica. 
El 90\% del corpus era de origen escrito y el 10\% restante surgió de grabaciones de conversaciones transcritas, de voluntarios de distintas edades, clases sociales y regiones. 
Estas conversaciones fueron producidas a partir de diferentes situaciones, algunas formales, como reuniones de gobierno, y otras más informales como programas de radio. 
Una de las grandes diferencias entre el BNC y los corpus ya existentes en ese momento, es que además de publicar los datos para investigaciones académicas, también se dio acceso a los datos para uso comercial y educativo.

El crecimiento de la cantidad de datos generados mediante sistemas informáticos en las últimas décadas fue tal que en el año 2003 Kilgariff et al. \cite{kilgarriff2003introduction} se preguntaron acerca de la posibilidad de utilizar la Web como fuente para recolectar textos.
La Web resulta una gran oportunidad para el estudio de las lenguas ya que provee una cantidad inmensa de datos, accesibles de forma gratuita y con disponibilidad inmediata. Hay varias críticas que se le pueden hacer al contenido que se encuentra en la web, como los errores sintácticos y ortográficos. Sin embargo, la inmensa cantidad de datos que es posible recolectar ofrece una oportunidad única para realizar estudios lingüísticos.

En particular, la lengua utilizada en las redes sociales nos brinda la posibilidad de identificar palabras muy asentadas en determinada región del español, que en muchos casos no llega nunca a publicarse en las fuentes tradicionales, como la prensa o la literatura. Esta dificultad proviene de varios factores. Por un lado, encontrar gran cantidad de autores nativos de diferentes lugares no es una tarea sencilla. Por otro lado, en la literatura se suele utilizar un vocabulario más restringido, normalmente excluyendo (o utilizando con menor frecuencia) términos del habla cotidiana. Un ejemplo de esto son los coloquialismos, cuyo uso es más frecuente en las redes sociales que en la literatura. 

La gran importancia de conocer el uso de las palabras en ciertas regiones se puede ver reflejado en las marcas geográficas (o diatópicas) que se encuentran en algunas entradas de los diccionarios. Esta información cobra importancia para saber, por ejemplo, si una palabra tiene un uso general o se la utiliza solamente en algunas regiones. El área de la lingüística que estudia los principios teóricos en que se basa la composición de diccionarios se conoce como lexicología. Históricamente se han hecho diccionarios hispanoamericanos comparando con el español que los diccionarios españoles, generalmente el diccionario de la Real Academia Española (Diccionario de la Lengua Española), consideran general \cite {zimmermann2006fin}. Esto ocurrió en parte por el gran desarrollo de los diccionarios de la lengua española desde principios del siglo XVIII y por el carácter incipiente de la lexicografía americana. Sin embargo, esta metodología que compara dialectos de países latinoamericanos con España tuvo críticas en las últimas décadas, especialmente porque \textcquote[p. 9]{avila2004fin}{una comparación adecuada es la que se puede establecer
entre los elementos de entidades equivalentes, como las que
forman los países.}(p. 9). Tanto Raúl Ávila como Klaus Zimmermann ponen en discusión el sentido de los diccionarios diferenciales de cada país, principalmente por no ser autosuficientes, ya que en un diccionario diferencial no se encuentran las palabras que se usan en ambas regiones, sino que aparecen únicamente los términos cuyo uso es mayor en la región a estudiar sobre la región con la que se compara. Ambos autores concluyen que es de mayor interés realizar diccionarios integrales, donde se marquen las palabras que se usan de forma contrastiva en una región, pero que también estén las palabras de uso general. Creemos que la metodología propuesta en esta tesis, facilitará el armado de estos diccionarios en particular y el estudio del léxico español hispanoamericano en general.\\

% TODO: contrastividad dicha por el proyecto de augsburgo
% Criterio de la contrastividad. En el nuevo diccionario sólo se incluirán unidades léxicas que o bien no se usan en el español peninsular o bien presentan diferencias en el uso americano frente al peninsular.
%TODO: FALTARÍA CONTEXTUALIZAR LOS CONTRASTES LEXICOS, EN ESPECIAL EL TRABAJO SOBRE EL ESPAÑOL (VIDAL DE BATTINI).

% \cite{baayen2001word} \\
% Lingüisitca de corpus


%Que es el trabajo que realizamos, y como lo presentamos

\section{Objetivo del estudio}

En este trabajo presentamos un método semi-supervisado para la detección de léxico contrastivo a través de un conjunto de textos recolectados de Twitter. Si bien se recolectaron textos de la Argentina, este trabajo puede ser replicado sobre otras regiones. Cabe mencionar que el método detecta palabras con valores significativos de contraste en su uso, y es necesaria la posterior supervisión de investigadores lexicógrafos entrenados para seleccionar los términos con interés lingüístico.

En la sección \ref{ch:datos} explicamos la metodología para extraer los datos de Twitter y presentamos la caracterización de la muestra. 
Luego en la sección \ref{ch:metricas} se muestran la métricas creadas para medir la contrastividad de una palabra y el análisis de estas.

En la sección \ref{ch:validacion} mostramos las palabras identificadas como más contrastivas a partir de la métrica elegida y la proporción acumulada de sus ocurrencias en regiones de pocas provincias. 
También detallamos la validación lingüística realizada por la Academia Argentina de Letras, exhibimos una caracterización de las palabras salientes y hacemos una validación estadística de la métrica a través de tests estadísticos. 

Finalmente en la sección \ref{ch:conclusiones} sacamos conclusiones a partir de los resultados obtenidos e indicamos trabajos posibles para seguir la investigación.

