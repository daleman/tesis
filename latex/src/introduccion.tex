
%Explicación del área
%el trabajo de vidal de batini y almeida
%Objetivo de la tesis

%Contrsuccion de lingüistica de corpus
%lingüistica computacional, su evolucion
%Todo se hace en ingles y hay pocos analisis en castellano.

%primer acercamiento de contrastes léxicos 


%Comentar el problema de la lingüística de Corpus, trabajo previo en el área, comentar brevemente qué queremos hacer. Acá va a ir mucho de lo que nos pasó Santiago como literatura, incluso mencionando (por ejemplo) el trabajo de Vidal de Battini
\subsection{Trabajo previo en el área}
\par Actualmente, las herramientas para la detección de palabras con contraste léxico en distintas regiones
consisten en cuestionarios como el de \emph{Almeida}\cite{almeida1995variacion}.  Estos cuestionarios están integrados por grupos temáticos centrales como la casa, la familia, la enseñanza, el cuerpo humano, etc. Sobre cada grupo temático se les indicaba a las personas entrevistadas un repertorio de palabras por cada noción, para ver si las conocían y con que frecuencia se las usaba. 
Con este trabajo planteamos cambiar el paradigma y detectar automáticamente las palabras usadas en distintas regiones y sus frecuencias.

Uno de los corpus lingüísticos del español más reconocidos es el \emph{corpes XXI}\cite{espanolabanco}, creado por la Real Academia Española con una distribución de 25 millones de formas por cada uno de los años comprendidos en el periodo 2001 
a 2012. Sin embargo, dicho corpus tiene dos desventajas importantes: por un
lado, la cantidad de palabras de América Latina están subrepresentados ya que el $65,70$\% de las palabras del dataset provienen de textos de España y $34,30$\% de los demás países hispanoparlantes. Por otro lado, uno no dispone de todo el dataset, sino que solamente se pueden hacer las consultas de su página web. Estas consultas están limitadas en cuanto a la cantidad de solicitudes y a las funcionalidades que estas proveen.

Una de las virtudes de hacer un corpus con un método de recolección de textos de forma automática se desprende de la cantidad superadora de longitud del corpus en comparación con métodos manuales como digitalización de textos.

A pesar de haber comenzado hace varias decadas la recolección de textos de la web para realizar corpus, no hay muchos en el idioma español.
Uno que se pudo encontrar es el de \textit{Mark Davies}, el cual se utilizó las páginas web para recolectar los textos, con dos billones de palabras en español y divide a las páginas en regiones a través de  

Las ventajas de Twitter son claras: da una interfaz pública para obtener tweets de cualquier persona. Además a diferencia de un portal de noticias donde los comentarios suelen estar relacionados con estas, en Twitter son más amplio los tópicos de los comentarios.
Por otro lado, Twitter es una tecnología que permite hacer escalabale el trabajo a diferentes paises ya que con una misma interfaz se pueden obtener los textos de cualquier región. En cambio, si se elige un portal de noticias para sacar comentarios de usuarios, deberíamos ver la estructura de cada página para obtener esos datos.
Otra ventaja de Twitter es que cada usuario está identificado y se podría llegar a inferir datos como género, edad y ubicación de cada uno.
Por último una ventaja sobre otro método de recolección, es que a través de Twitter se puede recolectar textos con una granularidad regional muy variable y obtener información de quienes lo escriben.
Existen otras redes sociales que se podrían obtener textos para analizar , pero tienen la desventaja de ser privadas como Facebook o ser acotadas en términos de los temas que se hablan en la misma, un caso de esto es Linkedin. 

Para dar una noción de la cantidad de usaurios en la Argentina:
En el 2016 había 11,8 millones de usuarios de Twitter en la Argentina, con 15 millones de personas con smartphones, lo cual el 70 de la gente con smartphones tenía Twitter.

\subsection{Lingüistica computacional} % (fold)
\label{sub:linguistica_computacional}

