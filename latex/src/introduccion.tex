
%Explicación del área
%el trabajo de vidal de batini y almeida
%Objetivo de la tesis

%Contrsuccion de lingüistica de corpus
%lingüistica computacional, su evolucion
%Todo se hace en ingles y hay pocos analisis en castellano.

%primer acercamiento de contrastes léxicos 


%Comentar el problema de la lingüística de Corpus, trabajo previo en el área, comentar brevemente qué queremos hacer. Acá va a ir mucho de lo que nos pasó Santiago como literatura, incluso mencionando (por ejemplo) el trabajo de Vidal de Battini

\section{Trabajo previo en el área}
La dialectología es un campo que estudia la variación del lenguaje según la región geográfica y el contexto social en los que se utiliza. La investigación cuantitativa en estos campo suele utilizar las frecuencias de variables lingüísticas: atributos fonéticos, sintácticos y léxicos.

%Ampliar sobre dialectologia (fijarse en la enciclopedia)

Actualmente, los métodos por los cuales se descubren palabras con contraste léxico en distintas regiones
consisten en cuestionarios como el de \emph{Almeida}\cite{almeida1995variacion}.  Estas encuestas están integradas por grupos temáticos centrales como la casa, la familia, la enseñanza, el cuerpo humano, etc. Sobre cada grupo temático se les indicaba a las personas entrevistadas un repertorio de palabras por cada noción, para ver si las conocían y con que frecuencia se las usaba. 
Con este trabajo planteamos cambiar el paradigma y detectar automáticamente las palabras usadas en distintas regiones y sus frecuencias.

Uno de los corpus lingüísticos del español más reconocidos es el \emph{corpes XXI}\cite{espanolabanco}, creado por la Real Academia Española con una distribución de 25 millones de formas por cada uno de los años comprendidos en el periodo 2001 
a 2012. Sin embargo, dicho corpus tiene dos desventajas importantes: por un
lado, la cantidad de palabras de América Latina están subrepresentados en relación a la demografía ya que el $34,30$\% de las palabras del dataset provienen de textos de España y $65,70$\% de los demás países hispanoparlantes. Por otro lado, uno no dispone de todo el dataset, sino que solamente se pueden hacer las consultas de su página web. Estas consultas están limitadas en cuanto a la cantidad de solicitudes y a las funcionalidades que estas proveen.

Una de las virtudes de hacer un corpus con un método de recolección de textos de forma automática se desprende del mayor tamaño del corpus en comparación con métodos manuales como digitalización de textos.

A pesar de haber comenzado hace varias décadas la recolección de textos de la web para realizar corpus, no hay muchos en el idioma español.
Uno es el de \textit{Mark Davies}, en el cual se utilizó las páginas web para recolectar los textos, con dos billones de palabras en español y dividió las páginas a partir del país de origen identificado por \textit{Google}.

Las ventajas de Twitter son claras: provee una interfaz pública para obtener tuits de cualquier persona, independientemente de que haya una relación con el usuario que escribió los tuits en la red social. Es decir, uno puede ver los tuits de otra persona, sin necesidad de ser un \textit{seguidor} de esta. Además a diferencia de un portal de noticias donde los comentarios suelen estar relacionados con estas, en Twitter son más amplios los tópicos de los comentarios.
Por otro lado, Twitter es una tecnología que permite hacer escalable el trabajo a diferentes países ya que con una misma interfaz se pueden obtener los textos de cualquier región. En cambio, si se elige un portal de noticias para sacar comentarios de usuarios, es necesario conocer la estructura de cada página para obtener esos datos.
Otra ventaja de Twitter es que cada usuario está identificado y se podrían llegar a inferir datos como género, edad y ubicación de cada uno.
Por último, una ventaja sobre otro método de recolección es que a través de Twitter se pueden recolectar textos con una granularidad regional muy variable y obtener información de quienes los escriben.
Existen otras redes sociales de las que se podrían obtener textos para analizar, pero tienen la desventaja de ser privadas,como \textit{Facebook}, o acotadas en términos de los temas que se hablan, como \textit{Linkedin}. 



\section{Twitter}
Twitter\footnote{www.twitter.com} es un servicio de microblogging creado en 2006. Los usuarios son variados, desde personas, instituciones gubernamentales y no gubernamentales hasta bots (i.e programa que corre tareas automáticamente). Cada usuario puede escribir textos llamados tuits, que tienen una longitud máxima de 140 caracteres. Las relaciones en Twitter no necesitan ser recíprocas, uno puede seguir a una persona, en cuyo caso va a poder leer todos los tuits generados por ella, como también puede ser seguido por una persona. Un tuit puede ser respondido, como también puede ser retuiteado. El retuit es un mecanismo para diseminar por la red tuits generados por otros usuarios. De esta manera si un usuario A realiza un retuit generado por el usuario B, cualquier seguidor de A también va a recibir ese tuit en su panel, al cual llamaremos \textit{timeline}. Si bien los tuits que se ven en el timeline son solo aquellos generados por los usuarios que uno sigue, todos los tuits son públicos, es decir que pueden ser accedidos a través de búsquedas en la plataforma.
Para dar una noción de la cantidad de usuarios en la Argentina, en el 2016 había 11,8 millones de usuarios de Twitter en la Argentina, con 15 millones de personas con smartphones, lo cual significa que el $70$\% de la gente con smartphones tenía Twitter.%{ref de lanacion}

En los últimos años se han publicado numerosos trabajos que utilizaron datos de Twitter, desde la detección y monitoreo de terremotos en tiempo real\cite{sakaki2010earthquake},análisis de sentimientos y de la opinión pública \cite{liu2012sentiment},  predecir el mercado de valores \cite{pak2010twitter} o los resultados de elecciones nacionales \cite{tumasjan2010predicting}. También se ha utilizado para localizar enfermedades por región \cite{paul2011you}.

En cuanto a trabajos relacionados con la lingüística cabe mencionar el trabajo de Eisenstein et al. \cite{eisenstein2010latent} en el que identifica palabras con una gran afinidad regional realizando un modelo probabilístico en el que asumen que las distribuciones léxicas dependen de la región geográfica y de una división de tópicos. Es decir que hay una división de temas sobre todo el dataset y dependiendo de la región del autor, este es más propenso a escribir con una variación dada. El mismo autor realizó un trabajo para identificar variables léxicas y detectar regiones dialectales \cite{eisenstein2014identifying}.
El trabajo de Gonçalves et al. \cite{gonccalves2014crowdsourcing} consistió en analizar las variaciones diatópicas de ciertos conceptos en las grandes ciudades hispanoparlantes. Utilizaron la técnica K-means \cite{bishop2006pattern} para obtener regiones dialectales. Por otro lado G. Doyle et al. \cite{doyle2014mapping} propone un método bayesiano para estimar la distribución de la frecuencia de una palabra (o frase) condicional a la ubicación de la persona que la escribe. 

% ......................................... TERMINAR


\section{Lingüística computacional} % (fold)
\label{linguistica_computacional}

La lingüística de corpus es una área que utiliza una serie de procedimientos o métodos para estudiar el lenguaje [\cite{mcenery2011corpus}]. Esta rama de la la lingüística intenta responder 
preguntas asociadas a la utilización del lenguaje a través de conjuntos de muestras de uso de la lengua. Aunque lo más común es que estas muestras provengan de textos, también se puede extraer datos a partir de grabaciones de voz o vídeos. Si bien esta rama nació realizando y analizando corpus de forma manual, el rápido crecimiento
tecnológico de las últimas décadas dio la posibilidad de tener corpus con millones de palabras y realizar algunos estudios de manera automatizada, utilizando menos recursos humanos y ahorrando tiempo. A continuación, haremos un breve resumen de la lingüística de corpus.

En el año 1967 se publicó el primer corpus con un millón de palabras denominado Brown Corpus (BNC), siendo así uno de los pioneros en la lingüística de corpus. Recién en el año 1995 se consiguió realizar un corpus del inglés británico con 100 millones de palabras. 
Este corpus fue una muestra con el objetivo de ser representativa del inglés británico de aquella época. 
Es importante destacar, por la gran cantidad de recursos que se utilizaron, que este trabajo se realizó con la colaboración de tres grandes editoriales, la Universidad de Oxford ,la Universidad de Lancaster y la Biblioteca Británica. 
El 90\% del corpus era de origen escrito y el 10\% restante sobre grabaciones de conversaciones transcritas, de voluntarios de distintas edades, clases sociales y regiones. 
Estas conversaciones fueron producidas a partir de diferentes situaciones, algunas formales como reuniones de gobierno y otras más informales como programas de radio. 
Una de las grandes diferencias entre el BNC y los corpus ya existentes en ese momento, es que además de publicar los datos para investigaciones académicas ,sino que también se dio acceso a los datos para uso comercial y educativo.

El crecimiento de la cantidad de datos generados mediante sistemas informáticos en las últimas décadas fue tal que en el año 2003 Kilgariff et al. \cite{kilgarriff2003introduction} se preguntaron acerca de la posibilidad de utilizar la Web como fuente para recolectar textos.
La Web resulta de una gran oportunidad para el estudio de las lenguas ya que provee una cantidad inmensa de datos, accesibles de forma gratuita y con disponibilidad inmediata. Hay varias críticas que se le pueden hacer al contenido que se encuentra en la web, como los errores sintácticos y  gramaticales. Sin embargo, la inmensa cantidad de datos que es posible recolectar ofrece una oportunidad única para realizar estudios lingüísticos.

%En el trabajo de investigación de Frank Keller y Mirella Lapata se concluyó que modelos probabílisticos basados en una gran cantidad de datos, incluso con ruido, son mejores que los que se basan en pequeños pero más limpios conjuntos de datos. Incluso se vio que las frecuencias de pares de palabras eran consistentes con otros conjuntos de datos de mayor tamaño y en algunos casos eran subrepresentadas (o no aparecían) en el corpus BNC.

En particular, la lengua utilizada en las redes sociales nos brinda la posibilidad de identificar palabras muy asentadas en determinada región del español, difícil de detectar manualmente en la literatura. Esta dificultad proviene de varios factores. Uno de ellos es que es difícil encontrar gran cantidad de autores nativos de diferentes lugares. Por otro lado, en la literatura se suele utilizar un vocabulario más restringido, normalmente excluyendo (o utilizando con menos frecuencia) términos del habla cotidiana. Por ejemplo los coloquialismos son usados de forma más común en las redes sociales que en la literatura. 

% Uno de los lugares donde se vuelcan los resultados de la lingüística de corpus 
La gran importancia de saber el uso de las palabras en ciertas regiones se puede ver reflejado en las marcas geográficas(o diatópicas) que se encuentran en los algunas de las entradas de diccionarios. El área de la lingüística que
%\textblock[TODO: CITAR RAE]{(...) estudia los principios teóricos en que se basa la composición de diccionarios} se conoce como lexicografía. Históricamente se han hecho diccionarios hispanoamericanos comparando con el español que el diccionario de la Real Academia Española (Diccionario de la Lengua Española) considera general.

%\citeblock[\cite zimmermann2006fin]{Aceptando esta ideología lingüística de la supremacía del español de España y de la concepción de la madre Academia se llega por consecuencia al documentar el léxico de Hispanoamérica únicamente en su diferencia al de España. Es ello la sociogénesis de la lexicografía diferencial predominante durante tanto tiempo en Hispanoamérica}
Podríamos dividir a los diccionarios hispanoamericanos en dos grandes grupos, aquellos diccionarios nacionales, o provinciales por un lado y los diccionarios continentales por el otro. Diversos autores, como es el caso de Klaus Zimmermann y Raúl Ávila han reconocido este hecho. 
% Ha habido un debate académico al respecto de los dicionarios contrastivos e integrales, en 

%Un proyecto lexicográfico puede tener distintos criterios. Es prevalente en la tradición la comparación con el español que el diccionario general de la Real Academia Española (Diccionario de la Lengua Española) considera general. Uno de los ejemplos más logrados basados en esa metodología es el proyecto de diccionarios nacionales de Günther Haensch y Reinhold Werner (a partir del año 2000, aparece incluso explicitado en el título de los diccionarios). El carácter incipiente de la lexicografía americana combinado con el innegable desarrollo del DLE como repertorio léxico desde principios del siglo XVIII hicieron un tanto obligatoria esa preferencia metodológica. Además, no había demasiadas alternativas. Todo era un cotejo con el DLE y si ese diccionario no incluía ningún diatópico en una palabra, entonces directamente se la consideraba como general y no se la incluía en ningún diccionario nacional, regional o de americanismos.

%La palabra "americanismo" requiere alguna aclaración. Así como un entrerrianismo es una palabra que se usa preponderantemente en el territorio de la provincia de Entre Ríos, un uruguayismo, en el territorio de Uruguay, etc., un "americanismo" es una palabra usada preponderantemente en el territorio de América. Nosotros estamos recopilando estos "americanismos" y, hasta ahora, no juntamos más de 5 (papa, tal vez anteojos, acaso cuadra como 'distancia entre dos esquinas'), es decir, palabras que en el Diccionario de Americanismos incluyen 17, 18 o 19 países. Esto es un indicador de que la categoría no tiene prácticamente ningún rendimiento descriptivo. Si uno hiciera una descripción de ese diccionario, sería algo así como: "diccionario que incluye la totalidad de las palabras regionales de la lengua española, excepto las de un país, España". Una región dialectal que abarca al 90% de los hablantes de español no sirve para nada. O mejor dicho, no sirve para nada lingüístico. Sirve sí el propósito de difundir la idea de que el español que se habla en España es "primario", "prioritario", "puro" o algo así. Además, existe un vacío absoluto de diccionarios de españolismos que contribuye a la misma idea.

%De modo que, respondiendo a tu pregunta, los criterios que se siguen pueden variar considerablemente y, en muchos casos, se basan en premisas que son directamente aberrantes (como postular que América es una región dialectal). Gran parte de la tradición debió basarse en lo hecho por los españoles porque no existía ninguna alternativa. Incluso hoy, con CORPES, es extremadamente limitado lo que se puede hacer fuera de la RAE con toda esa información. La siguiente etapa del trabajo que vos estás haciendo puede buscar ampliar la perspectiva a todos los países de habla hispana del mundo para detectar contrastes léxicos entre países. En principio, como hay aproximadamente el mismo número de provincias argentinas que de países hispanohablantes, se me ocurre que todo lo que tenemos avanzado hasta el momento va a poder aprovecharse para esa etapa con solo ajustar las geolocalizaciones. Creo que lograr eso sería un verdadero hito que va a interesar a todos los que estén en proyectos de lexicografía diferencial. 

%En cuanto a tu pregunta sobre las regiones lingüísticas, el proceso de definición suele basarse en identificar isoglosas. "América", por ejemplo, tiene un rechazo por el "vosotros" (algo que sucede en el sur de España también) y una lista de no más de 10 palabras. Algo a todas luces insuficiente. Normalmente se sigue un criterio nacional, que puede tener algo de fundamento en el hecho de que la cohesión institucional es una fuerza centrípeta de cierta importancia. En última instancia, depende de un tema de amplitud de la perspectiva y eso a su vez depende de los objetivos de un determinado proyecto. Nosotros ahora estamos interesados en las diferencias entre provincias y grupos de provincias. Cuando pasemos a la siguiente etapa, estaremos interesados en los países y los grupos de países. 

%A veces, un patrón de uso se ajusta a regiones previamente establecidas.
%Por ejemplo, chamigo, ga:
%Otras veces, esos patrones de uso tienen formas inesperadas.
%Como por ejemplo, manso, sa como adjetivo intensificador (del tipo "manso embole me estoy pegando"):


%En este caso se nos junta una provincia de litoral, dos de cuyo y una de pampa-patagonia. Esto significa que, con el debido análisis de los datos, podemos estar en condiciones de identificar patrones de uso​ diferentes, basados en la evidencia léxica que nos aporta Twitter.

%En cuanto a las encuestas, es una metodología muy frecuentada por su accesibilidad. Normalmente, un investigador no está en condiciones de hacer algo como lo que estamos haciendo nosotros. Eso lleva tiempo, conocimientos técnicos avanzados, colaboración entre instituciones, ajustes, aprendizaje. Lo que se hace entonces es diseñar un experimento y conseguir sujetos. En realidad, es sorprendente todo lo que se puede hacer por ese medio, pero también son muy importantes las limitaciones que impone el diseño del experimento (configurado inevitablemente por las premisas y preguntas del investigador) y la necesaria escasez de informantes. Conseguir más o menos 1000 usuarios por provincia y armar un corpus de cerca de 200 millones de palabras, como lograron ustedes, es absolutamente impensable en otros contextos. Normalmente, los diccionarios trabajan con otros diccionarios, identifican un lemario y después avanzan con investigaciones sobre casos específicos. Hoy estarían en condiciones de usar CORPES en esas investigaciones, como hacemos nosotros. Eso tiene sus problemas también por su carácter eminentemente escrito. Por ejemplo, la palabra chabón da para Argentina una FNorm de 2.3 en CORPES mientras que a nosotros nos da un promedio nacional de más de 200. En otros casos, como en payasesco, la palabra aparece como más usada en Panamá, pero eso está basado en un solo ejemplo que, además, proviene de una respuesta que da una actriz argentina. Un método sistemático de detección de contrastes léxicos basado en ejemplos reales de uso, como lo que estamos haciendo nosotros, no existe que yo sepa. Con Pedro especulamos que sería posible que la RAE hiciera algo así con su corpus, pero no sabemos si está en sus intereses o si pensaron en eso.

%Metodológicamente, entonces, estamos un poco en territorio no explorado y la bibliografía que se puede conseguir nunca va a dar cuenta de intentos similares (de vuelta, hasta donde yo sé). Existen estudios clásicos como la Historia de la lengua española de Rafael Lapesa o ​La América hispanohablante. Unidad y diferenciación del castellano de Bertil Malmberg, que te pueden aportar nociones lingüísticas e históricas de base, que siempre son útiles para conocer aunque sea superficialmente los rasgos generales de la diferenciación lingüística como fenómeno. Ambos los tenemos en la Academia si los necesitás.


TODO: FALTARÍA CONTEXTUALIZAR LOS CONTRASTES LEXICOS, EN ESPECIAL EL TRABAJO SOBRE EL ESPAÑOL (VIDAL DE BATTINI).

\cite{baayen2001word}
% Lingüisitca de corpus


%Que es el trabajo que realizamos, y como lo presentamos

En este trabajo presentamos un método semi-supervisado para la detección de palabras contrastivas a través de un conjunto de textos recolectados de Twitter. Si bien se recolectaron textos de la Argentina, este trabajo puede ser replicado sobre otras regiones. Cabe mencionar que la herramienta detecta palabras con valores significativos de contraste en su uso, es necesaria la supervisión de investigadores lexicógrafos entrenados para seleccionar los términos con interés lingüístico.

En la sección [\ref{ch:metodo}]
explicaremos la metodología para extraer los datos de Twitter, una caracterización de la muestra y la métrica creada para medir la contrastividad de una palabra.

En la sección [\ref{ch:resultados}] mostramos las palabras identificadas como más contrastivas y la proporción acumulada en regiones de pocas provincias. También detallamos los resultados del test estadístico sobre el conjunto de validación y los guaranismos detectados. 

Finalmente en la sección [\ref{ch:conclusiones}] sacamos conclusiones a partir de los resultados obtenidos e indicamos trabajos posibles para seguir la investigación.