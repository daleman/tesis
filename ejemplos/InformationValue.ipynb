{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Value\n",
    "\n",
    "En esta notebook vamos a testear Information Value definido por Zanette & Montemurro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from __future__ import division, print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords, gutenberg \n",
    "\n",
    "\n",
    "def is_punctuation(c):\n",
    "        return (len(c) == 1 and (c in \"-.'?!,\\\":;()|-/\")) or c == '\"\"' or  len(c) == 1 or c == '--' or c == ').' or c == '.\"\"' or c == ''\n",
    "    \n",
    "def tokenize(text, only_alpha = False, only_alphanum = True,  clean_stop_words = False, clean_punctuation = True):  \n",
    "    \"\"\"\n",
    "    Tokeniza text sacando alfanuméricos, stopwords y puntuación de ser necesario\n",
    "    \"\"\"\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    tokens = [t for t in tokens if  (not clean_punctuation or not is_punctuation(t)) \n",
    "        and (not only_alpha or t.isalpha())\n",
    "        and (not only_alphanum or t.isalnum())\n",
    "        and (not clean_stop_words or t not in stopwords.words('english'))]          \n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def get_moby_dick_tokens():\n",
    "    moby_dick = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')\n",
    "    tokens = tokenize(moby_dick, only_alphanum=True, clean_punctuation=True)\n",
    "    return [token.lower() for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = get_moby_dick_tokens()\n",
    "\n",
    "fd = nltk.FreqDist(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ocurrence_dataframe(tokenized_text, window_size):\n",
    "    \"\"\"\n",
    "    Construye una matriz de ocurrencias dado un tamaño de ventana\n",
    "    \"\"\"\n",
    "    freq = {}\n",
    "    \"\"\"\n",
    "    N = Largo del texto\n",
    "    P = Cantidad de ventanas\n",
    "    \"\"\"\n",
    "    N = len(tokenized_text)\n",
    "    P = int(math.ceil(N / window_size))\n",
    "\n",
    "    for i in range(0, P):\n",
    "        lower_bound, upper_bound = window_size * i, min(window_size* (i+1), N) \n",
    "        window = tokenized_text[lower_bound:upper_bound]\n",
    "        window_fdist = nltk.FreqDist(window)\n",
    "\n",
    "        for word in window_fdist:\n",
    "            if word not in freq:\n",
    "                freq[word] = [0] * P\n",
    "                \n",
    "            freq[word][i] = window_fdist[word]\n",
    "    return pd.DataFrame.from_dict(freq, orient=\"index\")\n",
    "\n",
    "occurrence_df = ocurrence_dataframe(md, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "occurrence_df[\"entropy\"] = occurrence_df.apply(entropy, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence_df.sort_values(by=\"entropy\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'funereal', u'dreamt', u'thickens', u'ramparts', u'spiralizations',\n",
       "       u'conflagration', u'prating', u'feegee', u'aboriginalness', u'overhung',\n",
       "       u'platters', u'repelling', u'preparative', u'fulller', u'pail',\n",
       "       u'oxygenated', u'tasks', u'normal', u'discreet', u'bestreaked',\n",
       "       u'thews', u'dugongs', u'scrupulously', u'sentence', u'overmanned',\n",
       "       u'poniards', u'identify', u'gauntleted', u'1775', u'1776',\n",
       "       u'shakespeare', u'belfast', u'1772', u'agile', u'defray', u'bladder',\n",
       "       u'fumes', u'trimming', u'beholds', u'enjoyments', u'pupella', u'claims',\n",
       "       u'conquering', u'bamboozle', u'salisbury', u'doctrine', u'armada',\n",
       "       u'stall', u'cones', u'snorts', u'crushing', u'jealousy', u'approximate',\n",
       "       u'brawniness', u'vacated', u'starve', u'marten', u'evinces',\n",
       "       u'entombment', u'placeless', u'analyse', u'scimetars', u'amounted',\n",
       "       u'formerly', u'subs', u'thinkest', u'upheaving', u'translated', u'cone',\n",
       "       u'mistifying', u'periodicalness', u'blackest', u'inquiringly',\n",
       "       u'bordering', u'missive', u'sylla', u'submarine', u'magazine',\n",
       "       u'crucifix', u'allotted', u'unsay', u'nerve', u'managers', u'wept',\n",
       "       u'gleam', u'practices', u'smoker', u'uno', u'scorch', u'motioning',\n",
       "       u'tilbury', u'exasperations', u'hygiene', u'springy', u'drilled',\n",
       "       u'ether', u'presumable', u'tint', u'isolatoes', u'patrolling'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurrence_df.index[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En nuestro caso...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from scipy.stats import entropy\n",
    "\n",
    "df = pd.read_csv(\"../contrastes/provincias.csv\", quotechar=\"\\\"\", decimal=\",\")\n",
    "\n",
    "df = df.set_index(\"palabra\")\n",
    "\n",
    "\n",
    "fnorm_vars = [c for c in df.columns if re.match(r'fnorm_.*', c)]\n",
    "cant_palabras = [c for c in df.columns if re.match(r'.*Palabras', c)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Método que calcula la entropía de una palabra (usando df)\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def w_entropy(df, word):\n",
    "    # Esto es porque se mambea con los tipos (creo)\n",
    "    vec = df.loc[word][cant_palabras].tolist()\n",
    "    \n",
    "    return entropy(vec)\n",
    "\n",
    "entropy([1] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hypergeom\n",
    "\n",
    "word = \"anga\"\n",
    "\n",
    "n = df.loc[word].cantPalabra\n",
    "N = df.cantPalabra.sum()\n",
    "P = 23 # cantidad de provincias\n",
    "\n",
    "hv = hypergeom(M=N, N=N/P, n=n)\n",
    "\n",
    "shuffled_entropy = -P * sum([hv.pmf(m) * (float(m)/n) * np.log2(float(m)/n) for m in xrange(1, min(n, N  / P))])\n",
    "\n",
    "print shuffled_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def expected_random_shuffle(df, word):\n",
    "    \"\"\"\n",
    "    Apéndice último del paper de Zanette\n",
    "    \"\"\"\n",
    "    n = df.loc[word].cantPalabra\n",
    "    N = df.cantPalabra.sum()\n",
    "    P = 23 # cantidad de provincias\n",
    "\n",
    "    hv = hypergeom(M=N, N=N/P, n=n)\n",
    "\n",
    "    return -P * sum([hv.pmf(m) * (float(m)/n) * np.log2(float(m)/n) for m in xrange(1, min(n, N  / P))])\n",
    "\n",
    "    \n",
    "\n",
    "def information_value(df, word):\n",
    "    \n",
    "    vec = df.loc[word][cant_palabras].tolist()\n",
    "    n = sum(vec)\n",
    "    \n",
    "    freq = float(df.loc[word].cantPalabra) / (df.cantPalabra.sum())\n",
    "    word_entropy = w_entropy(df, word)\n",
    "    \n",
    "    shuffled_entropy = expected_random_shuffle(df, word)\n",
    "\n",
    "    #print(word)\n",
    "    #print(\"entropía = {} entropía shuffle = {}\".format(word_entropy, shuffled_entropy))\n",
    "\n",
    "    res = np.log2(1+freq) * (shuffled_entropy - word_entropy) \n",
    "    \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [\n",
    "    \"anga\",\n",
    "    \"culiaw\",\n",
    "    \"despues\",\n",
    "    \"mitai\",\n",
    "    \"artante\",\n",
    "    \"q\",\n",
    "    \"como\",\n",
    "    \"ver\",\n",
    "    \"de\",\n",
    "    \"nah\"]\n",
    "\n",
    "for word in test_words:\n",
    "    print word, information_value(df,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"iv\"] = map(lambda word: information_value(df, word), df.index)\n",
    "df[\"entropy\"] = map(lambda word: w_entropy(df, word), df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.sort(columns=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df.cantPalabra > 150][[\"entropy\", \"iv\", \"cantPalabra\", \"provinciaFnormMax\"]].to_csv(\"prueba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
